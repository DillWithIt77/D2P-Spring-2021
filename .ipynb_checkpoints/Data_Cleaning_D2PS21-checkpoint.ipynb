{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data to Policy Spring 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weston Grewe and Angela Morrison\n",
    "\n",
    "University of Colorado Denver\n",
    "\n",
    "Math 7594 Integer Programming\n",
    "\n",
    "Instructor: Dr. Steffen Borgwardt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "A college degree is becoming a necessary requirement of entering the middle class. A college education is also an excellent way to lift people out of poverty. Some high schools have high immediate college enrollments while others have low immediate college enrollments. For any one high school, it is nearly impossible to determine which factors contribute significantly to college enrollment. Often, it is a blend of many factors such as class size, proportion of low income students, teacher pay, number of AP classes offered, and many other factors. \n",
    "\n",
    "In this project, we create an interpretable optimal decision tree to understand which factors make the greatest impact. We will use data from Massachusetts' public schools in 2017 which can be found on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('MA_Public_Schools_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 302 fields for 1861 schools. This includes elementary, middle, and high schools as well as schools that serve many grade levels. We will begin by selecting only schools which serve senior high school students. A school which does not serve seniors cannot have immediate college enrollment. We will then select only fields which would be beneficial to this analysis. For example, the number of AP classes taken is relevant while the school's principal is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1 = raw_data[raw_data['12_Enrollment'] > 1];\n",
    "slice1.columns.tolist();\n",
    "# slice1['AP_Test Takers'] = float(slice1['AP_Test Takers'])\n",
    "# # print(slice1['Average Expenditures per Pupil']);\n",
    "# print(np.array(slice1['AP_Test Takers']).max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must choose which columns to include in our analysis. It would also be interesting to study mutable and immutable factors in two different analyses to understand what changes. For instance, some factors may be most determining, e.g. poverty/wealth, but schools have no control over these factors. For a decision, schools can only be concerned with mutable factors, e.g. teacher pay, number of AP classes. \n",
    "\n",
    "Factors (in order of Col #)\n",
    "- School type (Public/Charter)\n",
    "- ZIP \n",
    "- District/District Code\n",
    "- Total Enrollment\n",
    "- First Lang Not English\n",
    "- English Lang Learner\n",
    "- Disability\n",
    "- High Need\n",
    "- Economically Disadvantaged\n",
    "- Race Makeup\n",
    "- Average Class Size\n",
    "- Average Salary\n",
    "- Average Expenditure per Pupil\n",
    "- % Graduated\n",
    "- % Dropped Out\n",
    "- AP Test takers\n",
    "- Number of Tests Taken\n",
    "- AP Score\n",
    "- Average SAT Math\n",
    "- Average SAT Reading\n",
    "- Average SAT Writing\n",
    "- 10th Grade MCAS (If used, filter for 10th graders)\n",
    "- Accountability Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace \"%\" symbols in column names to avoid possible errors in future column name calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice1.columns = slice1.columns.str.replace('%', 'Percent');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only columns with information we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> important_cols = slice1[['School Name', 'School Type','12_Enrollment', 'TOTAL_Enrollment', \n",
    "                             'First Language Not English','Percent First Language Not English','Students With Disabilities',\\\n",
    "                             'Percent Students With Disabilities', 'High Needs', 'Percent High Needs',\\\n",
    "                             'Economically Disadvantaged','Percent Economically Disadvantaged', \\\n",
    "                             'Percent African American', 'Percent Asian', 'Percent Hispanic', \\\n",
    "                             'Percent White','Percent Native American', 'Percent Native Hawaiian, Pacific Islander', \\\n",
    "                             'Percent Multi-Race, Non-Hispanic','Percent Males', 'Percent Females', \\\n",
    "                             'Average Class Size', 'Percent Graduated', 'Percent Non-Grad Completers','Percent GED',\\\n",
    "                             'Percent Dropped Out', 'High School Graduates (#)', 'Attending Coll./Univ. (#)', \\\n",
    "                             'Percent Attending College','Percent Private Two-Year', 'Percent Private Four-Year',\\\n",
    "                             'Percent Public Two-Year', 'Percent Public Four-Year', 'Percent MA Community College',\\\n",
    "                             'Percent MA State University','Percent UMass', 'AP_Test Takers', 'AP_Tests Taken',\\\n",
    "                             'AP_One Test', 'AP_Two Tests', 'AP_Three Tests','AP_Four Tests', 'AP_Five or More Tests',\\\n",
    "                             'AP_Score=1', 'AP_Score=2', 'AP_Score=3', 'AP_Score=4', 'AP_Score=5',\\\n",
    "                             'Percent AP_Score 1-2', 'Percent AP_Score 3-5', 'SAT_Tests Taken', \\\n",
    "                             'Average SAT_Reading', 'Average SAT_Writing', 'Average SAT_Math']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get current shape of new dataframe and remove rows with any missing data and get shape of new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(393, 54)\n",
      "(292, 54)\n"
     ]
    }
   ],
   "source": [
    "print(important_cols.shape)\n",
    "important_cols.isnull().sum().sum()\n",
    "clean_imp_cols = important_cols.dropna()\n",
    "print(clean_imp_cols.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print names of columns in cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_imp_cols.columns.tolist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown of non-binary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clean_imp_cols['TOTAL_Enrollment'].mean())\n",
    "# print(clean_imp_cols['Percent Students With Disabilities'].mean())\n",
    "# print(clean_imp_cols['Percent High Needs'].mean())\n",
    "# print(clean_imp_cols['Percent Economically Disadvantaged'].mean())\n",
    "# print(clean_imp_cols['Percent African American'].mean())\n",
    "# print(clean_imp_cols['Percent Asian'].mean())\n",
    "# print(clean_imp_cols['Percent Hispanic'].mean())\n",
    "# print(clean_imp_cols['Percent White'].mean())\n",
    "# print(clean_imp_cols['Percent Native American'].mean())\n",
    "# print(clean_imp_cols['Percent Native Hawaiian, Pacific Islander'].mean())\n",
    "# print(clean_imp_cols['Percent Multi-Race, Non-Hispanic'].mean())\n",
    "# print('Males = ',clean_imp_cols['Percent Males'].mean())\n",
    "# print('Females =',clean_imp_cols['Percent Females'].mean())\n",
    "# print('Average Class Size=',clean_imp_cols['Average Class Size'].mean())\n",
    "# print(clean_imp_cols['Percent Graduated'].mean())\n",
    "# print(clean_imp_cols['Percent Non-Grad Completers'].mean())\n",
    "# print(clean_imp_cols['Percent GED'].mean())\n",
    "# print(clean_imp_cols['Percent Dropped Out'].mean())\n",
    "# print('High School Grads =',clean_imp_cols['High School Graduates (#)'].mean())\n",
    "# print('Attending college =',clean_imp_cols['Percent Attending College'].mean())\n",
    "# #clean_imp_cols['AP_Test Takers'].mean()\n",
    "# #clean_imp_cols['AP_Tests Taken'].mean()\n",
    "# print('AP Test 1 = ',clean_imp_cols['AP_One Test'].mean())\n",
    "# print('AP Test 2 = ',clean_imp_cols['AP_Two Tests'].mean())\n",
    "# print('AP Test 3 = ',clean_imp_cols['AP_Three Tests'].mean())\n",
    "# print('AP Test 4 = ',clean_imp_cols['AP_Four Tests'].mean())\n",
    "# print('AP Test 5+ = ',clean_imp_cols['AP_Five or More Tests'].mean())\n",
    "# print(clean_imp_cols['AP_Score=1'].mean())\n",
    "# print(clean_imp_cols['AP_Score=2'].mean())\n",
    "# print(clean_imp_cols['AP_Score=3'].mean())\n",
    "# print(clean_imp_cols['AP_Score=4'].mean())\n",
    "# print(clean_imp_cols['AP_Score=5'].mean())\n",
    "# print(clean_imp_cols['Percent AP_Score 1-2'].mean())\n",
    "# print(clean_imp_cols['Percent AP_Score 3-5'].mean())\n",
    "# print('SAT Taken = ',clean_imp_cols['SAT_Tests Taken'].mean())\n",
    "# print(clean_imp_cols['Average SAT_Reading'].mean())\n",
    "# print(clean_imp_cols['Average SAT_Writing'].mean())\n",
    "# print(clean_imp_cols['Average SAT_Math'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for New Columns:\n",
    "* 0-33%, 33-66% and 66-110% for emost percetnage columns. \n",
    "* Outcome column cut off is 64.6% (anything higher is a 1, otherwise 0)\n",
    "* Average class size dived into 3rds also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_thirds_perc(column,first_third,middle_third,last_third):\n",
    "    for i in range(len(column)):\n",
    "        if column[i] <= 33.0:\n",
    "            first_third.append(1)\n",
    "            middle_third.append(0)\n",
    "            last_third.append(0)\n",
    "        elif (column[i] > 33.0 and column[i] <=66.0):\n",
    "            first_third.append(0)\n",
    "            middle_third.append(1)\n",
    "            last_third.append(0)\n",
    "        else:\n",
    "            first_third.append(0)\n",
    "            middle_third.append(0)\n",
    "            last_third.append(1)\n",
    "            \n",
    "    return first_third,middle_third,last_third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_outcome_bin(column,bin_outcome):\n",
    "    for i in range(len(column)):\n",
    "        if column[i] < 64.6:\n",
    "            bin_outcome.append(0)\n",
    "        else:\n",
    "            bin_outcome.append(1)\n",
    "    \n",
    "    return bin_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert columns to arrays for functions\n",
    "disab_array = np.array(clean_imp_cols['Percent Students With Disabilities'])\n",
    "high_needs_array = np.array(clean_imp_cols['Percent High Needs'])\n",
    "econ_dis_array = np.array(clean_imp_cols['Percent Economically Disadvantaged'])\n",
    "african_array = np.array(clean_imp_cols['Percent African American'])\n",
    "asian_array = np.array(clean_imp_cols['Percent Asian'])\n",
    "hispanic_array = np.array(clean_imp_cols['Percent Hispanic'])\n",
    "white_array = np.array(clean_imp_cols['Percent White'])\n",
    "native_array = np.array(clean_imp_cols['Percent Native American'])\n",
    "pacific_array = np.array(clean_imp_cols['Percent Native Hawaiian, Pacific Islander'])\n",
    "multi_race_array = np.array(clean_imp_cols['Percent Multi-Race, Non-Hispanic'])\n",
    "\n",
    "ap_test_taken_1_array = np.array(clean_imp_cols['AP_One Test'])\n",
    "ap_test_taken_2_array = np.array(clean_imp_cols['AP_Two Tests'])\n",
    "ap_test_taken_3_array = np.array(clean_imp_cols['AP_Three Tests'])\n",
    "ap_test_taken_4_array = np.array(clean_imp_cols['AP_Four Tests'])\n",
    "ap_test_taken_5_plus_array = np.array(clean_imp_cols['AP_Five or More Tests'])\n",
    "\n",
    "\n",
    "not_eng_array = np.array(clean_imp_cols['Percent First Language Not English'])\n",
    "\n",
    "\n",
    "\n",
    "#Create empty lists for conversion function\n",
    "disab_33_below = []\n",
    "disab_33_66 = []\n",
    "disab_66_above = []\n",
    "\n",
    "high_needs_33_below = []\n",
    "high_needs_33_66 = []\n",
    "high_needs_66_above = []\n",
    "\n",
    "econ_dis_33_below = []\n",
    "econ_dis_33_66 = []\n",
    "econ_dis_66_above = []\n",
    "\n",
    "african_33_below = []\n",
    "african_33_66 = []\n",
    "african_66_above = []\n",
    "\n",
    "asian_33_below = []\n",
    "asian_33_66 = []\n",
    "asian_66_above = []\n",
    "\n",
    "hispanic_33_below = []\n",
    "hispanic_33_66 = []\n",
    "hispanic_66_above = []\n",
    "\n",
    "white_33_below = []\n",
    "white_33_66 = []\n",
    "white_66_above = []\n",
    "\n",
    "native_33_below = []\n",
    "native_33_66 = []\n",
    "native_66_above = []\n",
    "\n",
    "pacific_33_below = []\n",
    "pacific_33_66 = []\n",
    "pacific_66_above = []\n",
    "\n",
    "multi_race_33_below = []\n",
    "multi_race_33_66 = []\n",
    "multi_race_66_above = []\n",
    "\n",
    "ap_test_taken_1_33_below = []\n",
    "ap_test_taken_1_33_66 = []\n",
    "ap_test_taken_1_66_above = []\n",
    "\n",
    "ap_test_taken_2_33_below = []\n",
    "ap_test_taken_2_33_66 = []\n",
    "ap_test_taken_2_66_above = []\n",
    "\n",
    "ap_test_taken_3_33_below = []\n",
    "ap_test_taken_3_33_66 = []\n",
    "ap_test_taken_3_66_above = []\n",
    "\n",
    "ap_test_taken_4_33_below = []\n",
    "ap_test_taken_4_33_66 = []\n",
    "ap_test_taken_4_66_above = []\n",
    "\n",
    "ap_test_taken_5_plus_33_below = []\n",
    "ap_test_taken_5_plus_33_66 = []\n",
    "ap_test_taken_5_plus_66_above = []\n",
    "\n",
    "not_eng_33_below = []\n",
    "not_eng_33_66 = []\n",
    "not_eng_66_above = []\n",
    "\n",
    "#Calling conversion fucntion\n",
    "disab_33_below,disab_33_66,disab_66_above = convert_to_thirds_perc(disab_array,disab_33_below,disab_33_66,disab_66_above);\n",
    "high_needs_33_below,high_needs_33_66,high_needs_66_above = convert_to_thirds_perc(high_needs_array,high_needs_33_below,high_needs_33_66,high_needs_66_above);\n",
    "econ_dis_33_below,econ_dis_33_66,econ_dis_66_above = convert_to_thirds_perc(econ_dis_array,econ_dis_33_below,econ_dis_33_66,econ_dis_66_above);\n",
    "african_33_below,african_33_66,african_66_above = convert_to_thirds_perc(african_array,african_33_below,african_33_66,african_66_above);\n",
    "asian_33_below,asian_33_66,asian_66_above = convert_to_thirds_perc(asian_array,asian_33_below,asian_33_66,asian_66_above);\n",
    "hispanic_33_below,hispanic_33_66,hispanic_66_above = convert_to_thirds_perc(hispanic_array,hispanic_33_below,hispanic_33_66,hispanic_66_above);\n",
    "white_33_below,white_33_66,white_66_above = convert_to_thirds_perc(white_array,white_33_below,white_33_66,white_66_above);\n",
    "native_33_below,native_33_66,native_66_above = convert_to_thirds_perc(native_array,native_33_below,native_33_66,native_66_above);\n",
    "pacific_33_below,pacific_33_66,pacific_66_above = convert_to_thirds_perc(pacific_array,pacific_33_below,pacific_33_66,pacific_66_above);\n",
    "multi_race_33_below,multi_race_33_66,multi_race_66_above = convert_to_thirds_perc(multi_race_array,multi_race_33_below,multi_race_33_66,multi_race_66_above);\n",
    "\n",
    "\n",
    "ap_test_taken_1_33_below,ap_test_taken_1_33_66,ap_test_taken_1_66_above = convert_to_thirds_perc(ap_test_taken_1_array,ap_test_taken_1_33_below,ap_test_taken_1_33_66,ap_test_taken_1_66_above);\n",
    "ap_test_taken_2_33_below,ap_test_taken_2_33_66,ap_test_taken_2_66_above = convert_to_thirds_perc(ap_test_taken_2_array,ap_test_taken_2_33_below,ap_test_taken_2_33_66,ap_test_taken_2_66_above);\n",
    "ap_test_taken_3_33_below,ap_test_taken_3_33_66,ap_test_taken_3_66_above = convert_to_thirds_perc(ap_test_taken_3_array,ap_test_taken_3_33_below,ap_test_taken_3_33_66,ap_test_taken_3_66_above);\n",
    "ap_test_taken_4_33_below,ap_test_taken_4_33_66,ap_test_taken_4_66_above = convert_to_thirds_perc(ap_test_taken_4_array,ap_test_taken_4_33_below,ap_test_taken_4_33_66,ap_test_taken_4_66_above);\n",
    "ap_test_taken_5_plus_33_below,ap_test_taken_5_plus_33_66,ap_test_taken_5_plus_66_above = convert_to_thirds_perc(ap_test_taken_5_plus_array,ap_test_taken_5_plus_33_below,ap_test_taken_5_plus_33_66,ap_test_taken_5_plus_66_above);\n",
    "\n",
    "not_eng_33_below,not_eng_33_66,not_eng_66_above = convert_to_thirds_perc(not_eng_array,not_eng_33_below,not_eng_33_66,not_eng_66_above);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert college percentage to binary outcomes\n",
    "outcome_array = np.array(clean_imp_cols['Percent Attending College'])\n",
    "outcome_bin = []\n",
    "\n",
    "outcome_bin = convert_outcome_bin(outcome_array,outcome_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-112-a209bcdf9c6d>:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_imp_cols[new_col_names[i]] = pd.DataFrame(columns_to_add[i], index=clean_imp_cols.index)\n"
     ]
    }
   ],
   "source": [
    "columns_to_add = [disab_33_below,disab_33_66,disab_66_above,high_needs_33_below,high_needs_33_66,\\\n",
    "                  high_needs_66_above,econ_dis_33_below,econ_dis_33_66,econ_dis_66_above,\\\n",
    "                  african_33_below,african_33_66,african_66_above,asian_33_below,asian_33_66,\\\n",
    "                  asian_66_above,hispanic_33_below,hispanic_33_66,hispanic_66_above,white_33_below,\\\n",
    "                  white_33_66,white_66_above,native_33_below,native_33_66,native_66_above,pacific_33_below,\\\n",
    "                  pacific_33_66,pacific_66_above,multi_race_33_below,multi_race_33_66,multi_race_66_above,\\\n",
    "                  ap_test_taken_1_33_below,ap_test_taken_1_33_66,ap_test_taken_1_66_above,ap_test_taken_2_33_below,\\\n",
    "                  ap_test_taken_2_33_66,ap_test_taken_2_66_above,ap_test_taken_3_33_below,ap_test_taken_3_33_66,\\\n",
    "                  ap_test_taken_3_66_above,ap_test_taken_4_33_below,ap_test_taken_4_33_66,ap_test_taken_4_66_above,\\\n",
    "                  ap_test_taken_5_plus_33_below,ap_test_taken_5_plus_33_66,ap_test_taken_5_plus_66_above,\\\n",
    "                  not_eng_33_below,not_eng_33_66,not_eng_66_above]\n",
    "\n",
    "new_col_names = ['Disability Percent 0-33','Disability Percent 33-66','Disability Percent 66-100',\\\n",
    "                 'High Needs Percent 0-33','High Needs Percent 33-66','High Needs Percent 66-100',\\\n",
    "                 'Econ Disadvantage Percent 0-33','Econ Disadvantage Percent 33-66','Econ Disadvantage 66-100',\\\n",
    "                 'African American 0-33','African American Percent 33-66','African American Percent 66-100',\\\n",
    "                 'Asian Percent 0-33','Asian Percent 33-66','Asian Percent 66-100','Hispanic Percent 0-33',\\\n",
    "                 'Hispanic Percent 33-66','Hispanic Percent 66-100','White Percent 0-33','White Percent 33-66',\\\n",
    "                 'White Percent 66-100','Native Percent 0-33','Native Percent 33-66','Native Percent 66-100',\\\n",
    "                 'Pacific Percent 0-33','Pacific Percent 33-66','Pacific Percent 66-100','Multi-Race Percent 0-33',\\\n",
    "                 'Multi-Race Percent 33-66','Multi-Race Percent 66-100','1 AP Test Percent 0-33',\\\n",
    "                 '1 AP Test Percent 33-66','1 AP Test Percent 66-100','2 AP Test Percent 0-33',\\\n",
    "                 '2 AP Test Percent 33-66','2 AP Test Percent 66-100','3 AP Test Percent 0-33',\\\n",
    "                 '3 AP Test Percent 33-66','3 AP Test Percent 66-100','4 AP Test Percent 0-33',\\\n",
    "                 '4 AP Test Percent 33-66','4 AP Test Percent 66-100','5+ AP Test Percent 0-33',\\\n",
    "                 '5+ AP Test Percent 33-66','5+ AP Test Percent 66-100','First Lang Not Eng Percent 0-33',\\\n",
    "                 'First Lang Not Eng Percent 33-66','First Lang Not Eng Percent 66-100']\n",
    "\n",
    "# clean_imp_cols = clean_imp_cols.reindex(columns=clean_imp_cols.columns.tolist() + new_col_names)\n",
    "\n",
    "for i in range(len(new_col_names)):\n",
    "    clean_imp_cols[new_col_names[i]] = pd.DataFrame(columns_to_add[i], index=clean_imp_cols.index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_imp_cols.columns.tolist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_imp_cols = clean_imp_cols.drop(columns=['Students With Disabilities','Percent Students With Disabilities','High Needs',\\\n",
    "                             'Percent High Needs','Economically Disadvantaged','Percent Economically Disadvantaged',\\\n",
    "                             'Percent African American','Percent Asian','Percent Hispanic','Percent White',\\\n",
    "                             'Percent Native American','Percent Native Hawaiian, Pacific Islander',\\\n",
    "                             'Percent Multi-Race, Non-Hispanic','Percent Males','Percent Females','Percent Graduated',\\\n",
    "                             'Percent Non-Grad Completers','Percent GED','Percent Dropped Out',\\\n",
    "                             'High School Graduates (#)','Attending Coll./Univ. (#)','Percent Attending College',\\\n",
    "                             'AP_One Test','AP_Two Tests','AP_Three Tests','AP_Four Tests','AP_Five or More Tests',\\\n",
    "                             'Percent First Language Not English','First Language Not English','Percent Private Two-Year',\\\n",
    "                             'Percent Private Four-Year','Percent Public Two-Year','Percent Public Four-Year',\\\n",
    "                             'Percent MA Community College','Percent MA State University','Percent UMass',\\\n",
    "                             'AP_Test Takers','AP_Tests Taken','AP_Score=1','AP_Score=2','AP_Score=3','AP_Score=4',\\\n",
    "                             'AP_Score=5','Percent AP_Score 1-2','Percent AP_Score 3-5','SAT_Tests Taken',\\\n",
    "                             'Average SAT_Reading','Average SAT_Writing','Average SAT_Math',]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['School Name',\n",
       " 'School Type',\n",
       " '12_Enrollment',\n",
       " 'TOTAL_Enrollment',\n",
       " 'Average Class Size',\n",
       " 'Disability Percent 0-33',\n",
       " 'Disability Percent 33-66',\n",
       " 'Disability Percent 66-100',\n",
       " 'High Needs Percent 0-33',\n",
       " 'High Needs Percent 33-66',\n",
       " 'High Needs Percent 66-100',\n",
       " 'Econ Disadvantage Percent 0-33',\n",
       " 'Econ Disadvantage Percent 33-66',\n",
       " 'Econ Disadvantage 66-100',\n",
       " 'African American 0-33',\n",
       " 'African American Percent 33-66',\n",
       " 'African American Percent 66-100',\n",
       " 'Asian Percent 0-33',\n",
       " 'Asian Percent 33-66',\n",
       " 'Asian Percent 66-100',\n",
       " 'Hispanic Percent 0-33',\n",
       " 'Hispanic Percent 33-66',\n",
       " 'Hispanic Percent 66-100',\n",
       " 'White Percent 0-33',\n",
       " 'White Percent 33-66',\n",
       " 'White Percent 66-100',\n",
       " 'Native Percent 0-33',\n",
       " 'Native Percent 33-66',\n",
       " 'Native Percent 66-100',\n",
       " 'Pacific Percent 0-33',\n",
       " 'Pacific Percent 33-66',\n",
       " 'Pacific Percent 66-100',\n",
       " 'Multi-Race Percent 0-33',\n",
       " 'Multi-Race Percent 33-66',\n",
       " 'Multi-Race Percent 66-100',\n",
       " '1 AP Test Percent 0-33',\n",
       " '1 AP Test Percent 33-66',\n",
       " '1 AP Test Percent 66-100',\n",
       " '2 AP Test Percent 0-33',\n",
       " '2 AP Test Percent 33-66',\n",
       " '2 AP Test Percent 66-100',\n",
       " '3 AP Test Percent 0-33',\n",
       " '3 AP Test Percent 33-66',\n",
       " '3 AP Test Percent 66-100',\n",
       " '4 AP Test Percent 0-33',\n",
       " '4 AP Test Percent 33-66',\n",
       " '4 AP Test Percent 66-100',\n",
       " '5+ AP Test Percent 0-33',\n",
       " '5+ AP Test Percent 33-66',\n",
       " '5+ AP Test Percent 66-100',\n",
       " 'First Lang Not Eng Percent 0-33',\n",
       " 'First Lang Not Eng Percent 33-66',\n",
       " 'First Lang Not Eng Percent 66-100']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_imp_cols.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save cleaned data to csv file to be used in AMPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_imp_cols.to_csv('clean_school_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_imp_cols.to_csv('clean_school_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
